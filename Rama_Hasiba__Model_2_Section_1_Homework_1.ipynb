{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramahasiba/Zakey/blob/main/Rama_Hasiba__Model_2_Section_1_Homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 Homework: Building Your First NLP Pipeline\n",
        "\n",
        "**Theory & Elaboration**\n",
        "This homework is your capstone for the week. You'll combine all the concepts we've discussed into a single, functional **NLP Pipeline**. This is one of the most common and essential tasks in all of Natural Language Processing.\n",
        "\n",
        "* **The NLP Pipeline (An Assembly Line for Text) 🏭**: Think of a pipeline as an assembly line for raw text. Messy, unstructured sentences go in one end, and each step on the line performs a specific, targeted transformation. The output of one step becomes the input for the next, until clean, structured, and useful data emerges at the end. The quality of your pipeline directly impacts the quality of your final analysis.\n",
        "\n",
        "* **The Goal: Preparing for a \"Bag of Words\"**: The ultimate goal of this cleaning process is to prepare our text for a machine learning model. The output of your pipeline—a clean list of root words—is the perfect input for a foundational model called the **Bag of Words (BoW)**.\n",
        "    * **Analogy**: Imagine you take a book, tear out all the pages, cut out every single word, and throw them all into a giant bag. You then shake the bag, disregarding all grammar and sentence order. Finally, you create a frequency count of every unique word. That's a Bag of Words!\n",
        "    * This simple frequency model is surprisingly powerful and is the basis for tasks like **document classification** (e.g., spam vs. not spam) and **sentiment analysis**. Your pipeline is the essential first step to creating this model.\n",
        "\n",
        "* **Why Order Matters**: As an engineer, the sequence of your pipeline is a critical design choice. A logical order is: `Lowercase -> Tokenize -> Filter -> Normalize`. This ensures, for example, that you lowercase words *before* checking them against an all-lowercase stop word list.\n",
        "\n",
        "**Your Challenge**\n",
        "Your task is to build a function, `preprocess_text`, that accepts a raw string of text. It must perform a series of normalization steps and return a list of cleaned, stemmed tokens, ready for a Bag of Words model.\n",
        "\n",
        "**Requirements & Hints:**\n",
        "* Your final list of tokens should be **lowercased**.\n",
        "* It should **not** contain any **punctuation**.\n",
        "* It should **not** contain any **stop words**.\n",
        "* Each word in the final list should be **stemmed** to its root form.\n",
        "* **Tools**: You will need to import and use `word_tokenize`, `stopwords` from `nltk`, and `string` from Python. You'll also need to initialize the `PorterStemmer`.\n",
        "* **Advanced Tip**: For a more elegant solution, see if you can perform the filtering and stemming steps inside a single list comprehension.\n",
        "\n",
        "**Bonus Challenge 🌟**\n",
        "Create a second function called `preprocess_with_lemma` that performs the same cleaning steps but uses **spaCy's lemmatization** instead of NLTK's stemming for the final normalization step.\n",
        "* **Hint**: You'll need to process the text with a spaCy `nlp` object to access the `.lemma_` attribute of each token. How can you integrate this with your existing filtering logic for stop words and punctuation?"
      ],
      "metadata": {
        "id": "BNQtyqftK8qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import all necessary libraries and functions.\n",
        "# You'll need tools for tokenization, stop words, and stemming from NLTK.\n",
        "# You'll also need the built-in 'string' library and 'spacy'.\\\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK packages for tokenization and stop words\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# --- Setup your tools ---\n",
        "# TODO: Get the set of English stop words.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# TODO: Get the set of all punctuation characters.\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# TODO: Create an instance of the PorterStemmer.\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# TODO: Load a spaCy model for the bonus challenge (e.g., 'en_core_web_sm').\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    This function takes raw text and returns a list of cleaned, stemmed tokens.\n",
        "    \"\"\"\n",
        "    # TODO: 1. Convert the text to lowercase and tokenize it.\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # TODO: 2. Filter out stop words and punctuation, then stem the remaining words.\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words and token not in punctuation:\n",
        "            cleaned_tokens.append(ps.stem(token))\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Bonus Challenge Function\n",
        "def preprocess_with_lemma(text):\n",
        "    \"\"\"\n",
        "    This function takes raw text and returns a list of cleaned, lemmatized tokens.\n",
        "    \"\"\"\n",
        "    # TODO: 1. Process the text with the loaded spaCy 'nlp' object.\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # TODO: 2. Loop through the tokens in the 'doc'. For each token, if it's not a stop word or punctuation, get its lowercase lemma.\n",
        "    cleaned_tokens_lemma = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() not in stop_words and token.text not in punctuation:\n",
        "            cleaned_tokens_lemma.append(token.lemma_)\n",
        "\n",
        "    return cleaned_tokens_lemma\n",
        "\n",
        "\n",
        "# --- Testing your functions ---\n",
        "test_text = \"Data Science is an amazing field! But, you'll need to clean your data first before analysis.\"\n",
        "\n",
        "# Test the main function\n",
        "processed_tokens = preprocess_text(test_text)\n",
        "print(\"--- Main Task (Stemming) ---\")\n",
        "print(\"Original Text:\", test_text)\n",
        "print(\"Processed Tokens:\", processed_tokens)\n",
        "\n",
        "test_sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "# Test the bonus function\n",
        "processed_lemma_tokens = preprocess_with_lemma(test_sentence)\n",
        "print(\"\\n--- Bonus Challenge (Lemmatization) ---\")\n",
        "print(\"Processed Tokens:\", processed_lemma_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuKwQBqYLDa3",
        "outputId": "9f775a05-d782-4efa-d34b-07ffa809ab96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Main Task (Stemming) ---\n",
            "Original Text: Data Science is an amazing field! But, you'll need to clean your data first before analysis.\n",
            "Processed Tokens: ['data', 'scienc', 'amaz', 'field', \"'ll\", 'need', 'clean', 'data', 'first', 'analysi']\n",
            "\n",
            "--- Bonus Challenge (Lemmatization) ---\n",
            "Processed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Self-Assessment\n",
        "\n",
        "Run the cell below to check your work. This script will evaluate the key exercises and the final homework to provide a score and detailed feedback. Make sure you have run all the cells above this one first."
      ],
      "metadata": {
        "id": "l8GKOdq2LTDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell to check your work\n",
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "def check_week5_nlp_tasks():\n",
        "    \"\"\"Checks the student's work for Week 5 and provides feedback.\"\"\"\n",
        "    score = 0\n",
        "    bonus_score = 0\n",
        "    total_points = 3\n",
        "    feedback = []\n",
        "\n",
        "\n",
        "    # --- Check 3: Homework (Stemming Pipeline) ---\n",
        "    try:\n",
        "        func_exists = 'preprocess_text' in globals()\n",
        "        if func_exists:\n",
        "            test_sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "            correct_output = ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "            student_output = preprocess_text(test_sentence)\n",
        "            if student_output == correct_output:\n",
        "                score += 1\n",
        "                feedback.append(\"- ✅ **Homework (Stemming):** Passed. The preprocessing pipeline function works correctly.\")\n",
        "            else:\n",
        "                feedback.append(f\"- ❌ **Homework (Stemming):** Failed. Your function did not produce the correct output. For a test sentence, expected `{correct_output}` but your function returned `{student_output}`.\")\n",
        "        else:\n",
        "            feedback.append(\"- ❌ **Homework (Stemming):** Failed. The function `preprocess_text` was not found.\")\n",
        "    except Exception as e:\n",
        "        feedback.append(f\"- ❌ **Homework (Stemming):** An error occurred: {e}\")\n",
        "\n",
        "    # --- Check 4: Bonus Challenge (Lemmatization Pipeline) ---\n",
        "    try:\n",
        "        func_exists = 'preprocess_with_lemma' in globals()\n",
        "        if func_exists:\n",
        "            test_sentence = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "            correct_output = ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
        "            student_output = preprocess_with_lemma(test_sentence)\n",
        "            if student_output == correct_output:\n",
        "                bonus_score += 1\n",
        "                feedback.append(\"- 🌟 **Bonus (Lemmatization):** Passed! The lemmatization pipeline works correctly. Excellent work!\")\n",
        "            else:\n",
        "                feedback.append(f\"- ⚠️ **Bonus (Lemmatization):** Found the function, but the output is incorrect. Expected `{correct_output}` but got `{student_output}`.\")\n",
        "    except Exception as e:\n",
        "        # Don't show an error if the bonus wasn't attempted\n",
        "        pass\n",
        "\n",
        "    # --- Final Feedback ---\n",
        "    final_message = \"**Week 5 Homework Self-Assessment Feedback:**\\n\\n\" + \"\\n\".join(feedback)\n",
        "    final_message += f\"\\n\\n### **Final Score: {score}/{total_points}**\"\n",
        "    if bonus_score > 0:\n",
        "        final_message += f\" (plus **{bonus_score}** bonus point!)\"\n",
        "\n",
        "    if score == total_points:\n",
        "        final_message += \"\\n\\nGreat job! All core tasks passed.\"\n",
        "    else:\n",
        "        final_message += \"\\n\\nSome tasks need revision. Please review the feedback above.\"\n",
        "\n",
        "    display(Markdown(final_message))\n",
        "\n",
        "check_week5_nlp_tasks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "OCVn7S1gLTph",
        "outputId": "52472816-13f7-435f-9936-4c713df21804"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Week 5 Homework Self-Assessment Feedback:**\n\n- ✅ **Homework (Stemming):** Passed. The preprocessing pipeline function works correctly.\n- 🌟 **Bonus (Lemmatization):** Passed! The lemmatization pipeline works correctly. Excellent work!\n\n### **Final Score: 1/3** (plus **1** bonus point!)\n\nSome tasks need revision. Please review the feedback above."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Pmqdzi-ybbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}